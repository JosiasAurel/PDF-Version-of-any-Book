{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5541c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d13b46c",
   "metadata": {},
   "source": [
    "# Libgen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc48556",
   "metadata": {},
   "source": [
    "v2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58ab098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4                                                            # Importing the Modules for Web-Scraping\n",
    "import pandas as pd                                                             \n",
    "from download import download\n",
    "\n",
    "author = \"Salman Rushdie\"                                                       # Name of the Author        \n",
    "author = author.split()                                                         # Splitting it between whitespaces so that later we can check it with\n",
    "                                                                                # the name of the author\n",
    "\n",
    "to_search = \"Midnight's Children\"                                               # Name of the Book\n",
    "pdf_title = to_search                                                           # Title for the PDF to be downloaded\n",
    "\n",
    "to_search_online = to_search.replace(\" \", \"+\")                                  # Replacing whitespaces with '+' for the link, to search on libgen\n",
    "\n",
    "link_to_search = f\"https://libgen.is/search.php?req={to_search_online}&open=0&res=100&view=simple&phrase=1&column=title\"        \n",
    "                                                                                # The link to search on libgen\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "list_of_dataframes = pd.read_html(link_to_search)                               # Adds the Table of the Webpage in a List\n",
    "\n",
    "table = list_of_dataframes[2]                                                   # DataFrame of Books from the List\n",
    "\n",
    "a = list(table.loc[0])                                                          # Placing the Contents of the 1st Row in 'a'\n",
    "\n",
    "table.columns = a                                                               # Changing the Names of the Columns\n",
    "\n",
    "table.drop(0 , axis = 0, inplace = True)                                        # Dropping the 1st Row of the DataFrame\n",
    "\n",
    "table.reset_index(inplace = True)                                               # Making the Index start from 0 to 99 \n",
    "                                                                                # So it has 100 books in the table\n",
    "\n",
    "table = table[(table['Extension'] == 'pdf')]                                    # Taking all the Books which are in the PDF format\n",
    "\n",
    "table.sort_values('Year', ascending = False, inplace = True)                    # Sorting the Books in the Descending, so the Latest Book is on \n",
    "                                                                                # the Top\n",
    "\n",
    "table_2 = table[['Author(s)', 'Title']]                                         # A new table which consists of the 'Name of the Author' \n",
    "                                                                                # and 'Title of the Books'\n",
    "\n",
    "index_number = 0                                                                # For getting the 'Number of the Book' to be downloaded\n",
    "                                                                                # Here, 'Number of the Book' can be 1 or 2 or 3 or 4......\n",
    "                                                                                \n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def check_for_author(index_for_author):                                         # For Matching the 'Name of the Author given by the user' \n",
    "                                                                                # and the 'Name of the Author of the Books from Libgen'  \n",
    "    \n",
    "    actual_author_name = table_2['Author(s)'][index_for_author]                 # Name of the Author of the Book from Libgen\n",
    "    result = False                                                              # Setting it to false so at the first match we can break the loop                      \n",
    "    for i in author :                                                           # Looping through the Name of the Author given by the user\n",
    "        if actual_author_name.find(i) > -1 :                                    # Returns a Number >= 0 if it finds the input given\n",
    "            result = True                                                       \n",
    "            break           \n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def check_for_title(title_of_book):                                             # For Matching the 'Title of the Book given by the user' \n",
    "                                                                                # and the 'Title of the Books from Libgen' \n",
    "    len_of_book_searched = len(to_search.split(\" \"))                            # Length of the List of Words in the Title given by the user\n",
    "    \n",
    "    split_of_book_by_libgen = title_of_book.lower().split(\" \")                  # List of Words in the Title given by Libgen\n",
    "\n",
    "    no_of_matches = 0                                                           # Variable for the matches of Titles given by user and Libgen\n",
    "\n",
    "    for i in range(0, len_of_book_searched):                                    # Looping through the List of Words in the Title given by Libgen\n",
    "        if(split_of_book_by_libgen[i].lower() in to_search.lower() ):           # Checking if the Word is in the Title given by the user\n",
    "            no_of_matches += 1                                                  # On a successful match, we increase the number of matches\n",
    "    return (no_of_matches == len_of_book_searched)                              # Returns the Boolean of the match\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "book_found = False\n",
    "\n",
    "for i in table_2.index :                                                            # Looping through the DataFrame which has 'Author' & 'Title' Columns\n",
    "    if((table_2['Title'][i].find(to_search) == 0 ) & (check_for_author(i))) :       # Checking if the Title Given by the User is \n",
    "                                                                                    # in the Title Given by Libgen\n",
    "        book_found = True\n",
    "        index_number = i                                                            # Setting the i'th index to index_number\n",
    "        break\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "if(book_found):\n",
    "\n",
    "    result = requests.get(link_to_search)                                           # Getting the Info about the Libgen Page, which the User searched\n",
    "\n",
    "    soup = bs4.BeautifulSoup(result.text,\"lxml\")                                    # Beautfiul Soup Instance \n",
    "\n",
    "    a = soup.select(\".c > tr > td > a\")                                             # List of the Anchor Elements from the Webpage Source Code\n",
    "\n",
    "    c = []                                                                          # List for Unique Number('Codes') of the \n",
    "                                                                                    # 'URL Links' of the Download Pages\n",
    "\n",
    "    for i in a :                                                                    # Looping through the Anchor Elements of the Webpage \n",
    "        b = i.get('href')                                                           # Getting the URL('href' attribute) of the Anchor Element \n",
    "        if(b.find(\"book\") == 0):                                                    # Checking if the URL starts with the word 'book'\n",
    "            c.append(b.split(\"=\")[1])                                               # Splitting it between '=' and getting the Unique Number('Code')\n",
    "                                                                                    # On observation, we decide to take the 2nd element \n",
    "                                                                                    # of the Splitted String                                     \n",
    "\n",
    "    index_of_book_to_download = c                                                   \n",
    "\n",
    "    link_of_download_page = f\"http://library.lol/main/{index_of_book_to_download[index_number]}\"    # The Download Page for the Chosen Book\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    result_2 = requests.get(link_of_download_page)                                  # Getting the Info about the Download Page, which the Code chose\n",
    "\n",
    "    soup = bs4.BeautifulSoup(result_2.text,\"lxml\")                                  # Beautfiul Soup Instance  \n",
    "\n",
    "    d = soup.select(\"#download > h2 > a\")                                           # List of the Anchor Elements from the Webpage Source Code \n",
    "\n",
    "    download_link = d[0].get('href')                                                # Getting the URL('href' attribute) of the Anchor Element which \n",
    "                                                                                    # is our download link\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    url       = download_link                                                       # Setting the url to the download link\n",
    "\n",
    "    file_path = \"Downloads/example1000.pdf\"                                         # Saving the PDF in Downloads Folder as example1000 \n",
    "\n",
    "    path = download(url, file_path, replace=True, kind=\"file\", timeout = 300.0)     # Downloading the Pdf\n",
    "\n",
    "else :\n",
    "    \n",
    "    print(\"This Book is not available on Libgen\")                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eb4ad0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a95b25a",
   "metadata": {},
   "source": [
    "v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a4aa2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AbhishekKonge\\AppData\\Local\\Temp\\ipykernel_4272\\1351156946.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  table.sort_values('Year', ascending = False, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "import requests, bs4                                                            # Importing the Modules for Web-Scraping\n",
    "import pandas as pd                                                             \n",
    "\n",
    "author = \"Mark Ryan\"                                                            # Name of the Author        \n",
    "author = author.split()                                                         # Splitting it between whitespaces so that later we can check it with\n",
    "                                                                                # the name of the author\n",
    "\n",
    "to_search = \"Calculus for Dummies\"                                              # Name of the Book\n",
    "pdf_title = to_search                                                           # Title for the PDF to be downloaded\n",
    "\n",
    "to_search_online = to_search.replace(\" \", \"+\")                                  # Replacing whitespaces with '+' for the link, to search on libgen\n",
    "\n",
    "link_to_search = f\"https://libgen.is/search.php?req={to_search_online}&open=0&res=100&view=simple&phrase=1&column=title\"        \n",
    "                                                                                # The link to search on libgen\n",
    "\n",
    "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "list_of_dataframes = pd.read_html(link_to_search)                               # Adds the Table of the Webpage in a List\n",
    "\n",
    "table = list_of_dataframes[2]                                                   # DataFrame of Books from the List\n",
    "\n",
    "a = list(table.loc[0])                                                          # Placing the Contents of the 1st Row in 'a'\n",
    "\n",
    "table.columns = a                                                               # Changing the Names of the Columns\n",
    "\n",
    "table.drop(0 , axis = 0, inplace = True)                                        # Dropping the 1st Row of the DataFrame\n",
    "\n",
    "table.reset_index(inplace = True)                                               # Making the Index start from 0 to 99 \n",
    "                                                                                # So it has 100 books in the table\n",
    "\n",
    "table = table[(table['Extension'] == 'pdf')]                                    # Taking all the Books which are in the PDF format\n",
    "\n",
    "table.sort_values('Year', ascending = False, inplace = True)                    # Sorting the Books in the Descending, so the Latest Book is on \n",
    "                                                                                # the Top\n",
    "\n",
    "table_2 = table[['Author(s)', 'Title']]                                         # A new table which consists of the 'Name of the Author' \n",
    "                                                                                # and 'Title of the Books'\n",
    "\n",
    "index_number = 0                                                                # For getting the 'Number of the Book' to be downloaded\n",
    "                                                                                # Here, 'Number of the Book' can be 1 or 2 or 3 or 4......\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def check_for_author(index_for_author):                                         # For Matching the 'Name of the Author given by the user' \n",
    "                                                                                # and the 'Name of the Author of the Books from Libgen'  \n",
    "    \n",
    "    actual_author_name = table_2['Author(s)'][index_for_author]                 # Name of the Author of the Book from Libgen\n",
    "    result = False                                                              # Setting it to false so at the first match we can break the loop                      \n",
    "    for i in author :                                                           # Looping through the Name of the Author given by the user\n",
    "        if actual_author_name.find(i) > -1 :                                    # Returns a Number >= 0 if it finds the input given\n",
    "            result = True                                                       \n",
    "            break           \n",
    "    return result\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "for i in table_2.index :                                                        # Looping through the DataFrame which has 'Author' & 'Title' Columns\n",
    "    if((table_2['Title'][i].find(to_search) == 0 ) & (check_for_author(i))) :   # Checking if the Title Given by the User is \n",
    "                                                                                # in the Title Given by Libgen\n",
    "        \n",
    "        index_number = i                                                        # Setting the i'th index to index_number\n",
    "        break\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "result = requests.get(link_to_search)                                           # Getting the Info about the Libgen Page, which the User searched\n",
    "\n",
    "soup = bs4.BeautifulSoup(result.text,\"lxml\")                                    # Beautfiul Soup Instance \n",
    "\n",
    "a = soup.select(\".c > tr > td > a\")                                             # List of the Anchor Elements from the Webpage Source Code\n",
    "\n",
    "c = []                                                                          # List for Unique Number('Codes') of the \n",
    "                                                                                # 'URL Links' of the Download Pages\n",
    "\n",
    "for i in a :                                                                    # Looping through the Anchor Elements of the Webpage \n",
    "    b = i.get('href')                                                           # Getting the URL('href' attribute) of the Anchor Element \n",
    "    if(b.find(\"book\") == 0):                                                    # Checking if the URL starts with the word 'book'\n",
    "        c.append(b.split(\"=\")[1])                                               # Splitting it between '=' and getting the Unique Number('Code')\n",
    "                                                                                # On observation, we decide to take the 2nd element \n",
    "                                                                                # of the Splitted String                                     \n",
    "\n",
    "index_of_book_to_download = c                                                   \n",
    "\n",
    "link_of_download_page = f\"http://library.lol/main/{index_of_book_to_download[index_number]}\"    # The Download Page for the Chosen Book\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "result_2 = requests.get(link_of_download_page)                                  # Getting the Info about the Download Page, which the Code chose\n",
    "\n",
    "soup = bs4.BeautifulSoup(result_2.text,\"lxml\")                                  # Beautfiul Soup Instance  \n",
    "\n",
    "d = soup.select(\"#download > h2 > a\")                                           # List of the Anchor Elements from the Webpage Source Code \n",
    "\n",
    "download_link = d[0].get('href')                                                # Getting the URL('href' attribute) of the Anchor Element which \n",
    "                                                                                # is our download link\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "r = requests.get(download_link, stream = True)                                  # 'Request' Instance for the Download Link\n",
    "\n",
    "with open(f\"Downloads/{pdf_title}.pdf\",\"wb\") as pdf_download:                   # Making a PDF in the Downloads Folder with \n",
    "                                                                                # the Title given by the User\n",
    "    for chunk in r.iter_content(chunk_size=100000):                             # Pieces of Information \n",
    "        if (chunk):                                                             # If the Piece of Information exists\n",
    "            pdf_download.write(chunk)                                           # Adding the Piece of Information to our PDF in the Downloads Folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368a3b98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a6914af",
   "metadata": {},
   "source": [
    "# PDF Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a99543e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For PDF DRIVE\n",
    "\n",
    "#search_link = 'https://www.pdfdrive.com/{...................}-books.html'\n",
    "\n",
    "\n",
    "\n",
    "import requests, bs4, webbrowser\n",
    "import pandas as pd\n",
    "\n",
    "author = \"Meg Waite Clayton\"\n",
    "author = author.split()\n",
    "to_search = \"The Last Train To London\"\n",
    "pdf_title = to_search\n",
    "to_search_online = to_search.replace(\" \", \"-\")\n",
    "link_to_search = f\"http://www.pdfdrive.com/{to_search_online}-books.html\"\n",
    "\n",
    "\n",
    "\n",
    "result = requests.get(link_to_search)\n",
    "\n",
    "soup = bs4.BeautifulSoup(result.text,\"lxml\")\n",
    "\n",
    "name_of_books       = soup.select(\".ai-search > h2\")\n",
    "year_of_books       = soup.select(\".file-info > .fi-year \")\n",
    "downloads_of_books   = soup.select(\".file-info > .fi-hit\")\n",
    "to_make_download_links = soup.select(\".file-right > a \")\n",
    "\n",
    "\n",
    "\n",
    "a = []\n",
    "b = []\n",
    "\n",
    "for x in name_of_books :\n",
    "    a.append(x.getText())\n",
    "\n",
    "for y in downloads_of_books : \n",
    "    b.append(int(y.getText().split(\" \")[0].replace(',','')))\n",
    "    \n",
    "\n",
    "    \n",
    "df_book  = pd.DataFrame(data = zip(a,b), columns = ['Book', 'Downloads'])\n",
    "\n",
    "df_book.sort_values('Downloads', ascending= False, inplace = True)\n",
    "\n",
    "\n",
    "index_of_book = 0\n",
    "\n",
    "for x in df_book.index:\n",
    "    if(df_book.loc[x]['Book'].find(to_search) == 0) :\n",
    "        index_of_book = x\n",
    "        break\n",
    "\n",
    "raw_download_link = to_make_download_links[index_of_book].get('href')\n",
    "\n",
    "download_link = raw_download_link[::-1].replace('e', 'd', 1)[::-1]\n",
    "download_link\n",
    "\n",
    "link_of_download_page = f\"http://www.pdfdrive.com{download_link}\"\n",
    "# link_of_download_page\n",
    "\n",
    "\n",
    "webbrowser.open(link_of_download_page)  # Go to example.com\n",
    "\n",
    "# result_2 = requests.get(link_of_download_page)\n",
    "\n",
    "# soup = bs4.BeautifulSoup(result_2.text,\"lxml\")\n",
    "\n",
    "\n",
    "# d = soup.select(\".dialog > .dialog-main > .dialog-left > .ebook-main\")\n",
    "# d\n",
    "# download_link = \"\"\n",
    "\n",
    "# for x in d:\n",
    "#     print(x.get('href'))\n",
    "# #     if (x.get('href').find(\"/download\") == 0):\n",
    "# #         download_link = x.get('href')\n",
    "# #         print x.get('href')\n",
    "# #         break\n",
    "# # download_link\n",
    "    \n",
    "# download_link = d[0].get('href')\n",
    "\n",
    "# r = requests.get(\"https://www.pdfdrive.com/download.pdf?id=156852032&h=11fba076a6384bbf99bc5765d63b64af&u=cache&ext=pdf\", stream = True)\n",
    "\n",
    "\n",
    "# with open(f\"Downloads/{pdf_title}.pdf\",\"wb\") as pdf:\n",
    "#     for chunk in r.iter_content(chunk_size=1000000):\n",
    "  \n",
    "#          if chunk:\n",
    "#              pdf.write(chunk)\n",
    "# https://www.pdfdrive.com/henry-huggins-d185567187.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a96457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0d66d06d",
   "metadata": {},
   "source": [
    "# ZLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5456915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://b-ok.asia/s/Harry%20Potter%20and%20the%20Deathly%20Hallows%20J.%20K.%20Rowling/?languages%5B0%5D=english&extensions%5B0%5D=pdf'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import requests, bs4\n",
    "import pandas as pd\n",
    "\n",
    "author = \"J. K. Rowling\"\n",
    "# author = author.split()\n",
    "to_search = \"Harry Potter and the Deathly Hallows\"\n",
    "pdf_title = to_search\n",
    "to_search_online = to_search.replace(\" \", \"%20\") + \"%20\" + author.replace(\" \", \"%20\")\n",
    "# link_to_search = f\"https://en.de1lib.org/s/{to_search_online}/?languages%5B0%5D=english&extensions%5B0%5D=pdf\"\n",
    "link_to_search = f\"https://b-ok.asia/s/{to_search_online}/?languages%5B0%5D=english&extensions%5B0%5D=pdf\"\n",
    "author = author.split()\n",
    "\n",
    "link_to_search\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# list_of_dataframes = pd.read_html(link_to_search)\n",
    "# table = list_of_dataframes[2]\n",
    "# # table\n",
    "# a = list(table.loc[0])\n",
    "\n",
    "# table.columns = a\n",
    "# table.drop(0 , axis = 0, inplace = True)\n",
    "# table.reset_index(inplace = True)\n",
    "# table = table[(table['Extension'] == 'pdf')]\n",
    "# # table\n",
    "# table.sort_values('Year', ascending = False, inplace = True)\n",
    "# table_2 = table[['Author(s)', 'Title']]\n",
    "# # table_2\n",
    "# index_number = 0\n",
    "\n",
    "# def check_for_author(index_for_author):\n",
    "#     actual_author_name = table_2['Author(s)'][index_for_author] \n",
    "#     result = False\n",
    "#     for i in author :\n",
    "#         if actual_author_name.find(i) > -1 :\n",
    "#             result = True\n",
    "#             break\n",
    "#     return result\n",
    "\n",
    "# for i in table_2.index :\n",
    "#     if((table_2['Title'][i].find(to_search) == 0 ) & (check_for_author(i))) :\n",
    "#         index_number = i\n",
    "#         break\n",
    "\n",
    "# my_dict = dict(table['Title'])\n",
    "# for i in my_dict :\n",
    "#     if(my_dict[i].find(to_search) == 0):\n",
    "#         index_number = i\n",
    "#         break\n",
    "\n",
    "# result = requests.get(link_to_search)\n",
    "\n",
    "# soup = bs4.BeautifulSoup(result.text,\"lxml\")\n",
    "\n",
    "# a = soup.select(\".c > tr > td > a\")\n",
    "\n",
    "# c = []\n",
    "# for i in a :\n",
    "#     b = i.get('href')\n",
    "#     if(b.find(\"book\") == 0):\n",
    "#         c.append(b.split(\"=\")[1])\n",
    "\n",
    "# index_of_book_to_download = c\n",
    "# link_of_download_page = f\"http://library.lol/main/{index_of_book_to_download[index_number]}\"\n",
    "\n",
    "# result_2 = requests.get(link_of_download_page)\n",
    "\n",
    "# soup = bs4.BeautifulSoup(result_2.text,\"lxml\")\n",
    "\n",
    "# d = soup.select(\"#download > h2 > a\")\n",
    "\n",
    "# download_link = d[0].get('href')\n",
    "\n",
    "# r = requests.get(download_link, stream = True)\n",
    "\n",
    "\n",
    "# with open(f\"Downloads/{pdf_title}.pdf\",\"wb\") as pdf:\n",
    "#     for chunk in r.iter_content(chunk_size=1000000):\n",
    "  \n",
    "#          if chunk:\n",
    "#              pdf.write(chunk)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# result_2 = requests.get(\"https://b-ok.asia/book/882239/c4d172?dsource=mostpopular\")\n",
    "\n",
    "# soup = bs4.BeautifulSoup(result_2.text,\"lxml\")\n",
    "\n",
    "# d = soup.select(\".row\")\n",
    "\n",
    "# urls = []\n",
    "# for link in soup.find_all('a'):\n",
    "#     print(link.get('href'))\n",
    "# download_link = d[0].get('href')\n",
    "\n",
    "# download_link\n",
    "\n",
    "# download_link = \"https://b-ok.asia/dl/5214617/5c2517\"\n",
    "\n",
    "# r = requests.get(download_link, stream = True)\n",
    "\n",
    "\n",
    "# pdf_title = \"Some_Book_From_Zlib_22222\"\n",
    "\n",
    "# with open(f\"Downloads/{pdf_title}.pdf\",\"wb\") as pdf:\n",
    "#     for chunk in r.iter_content(chunk_size=1000000):\n",
    "  \n",
    "#          if chunk:\n",
    "#              pdf.write(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ac371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d4e1c18",
   "metadata": {},
   "source": [
    "# Mangareader to Email "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c25ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c7f1a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdfkit\n",
    "\n",
    "config = pdfkit.configuration(wkhtmltopdf=r\"C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe\")\n",
    "                             \n",
    "pdfkit.from_url('https://mangareader.to/read/one-piece-3/en/chapter-1033','mangaopenpiece-1033.pdf', configuration = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327b4046",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd2edb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
